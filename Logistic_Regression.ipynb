{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwmishra/andol/blob/master/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9qBjoWDJVNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "  \n",
        "def loadCSV(filename): \n",
        "    ''' \n",
        "    function to load dataset \n",
        "    '''\n",
        "    with open(filename,\"r\") as csvfile: \n",
        "        lines = csv.reader(csvfile) \n",
        "        dataset = list(lines) \n",
        "        for i in range(len(dataset)): \n",
        "            dataset[i] = [float(x) for x in dataset[i]]      \n",
        "    return np.array(dataset) \n",
        "  \n",
        "  \n",
        "def normalize(X): \n",
        "    ''' \n",
        "    function to normalize feature matrix, X \n",
        "    '''\n",
        "    mins = np.min(X, axis = 0) \n",
        "    maxs = np.max(X, axis = 0) \n",
        "    rng = maxs - mins \n",
        "    norm_X = 1 - ((maxs - X)/rng) \n",
        "    return norm_X \n",
        "  \n",
        "  \n",
        "def logistic_func(beta, X): \n",
        "    ''' \n",
        "    logistic(sigmoid) function \n",
        "    '''\n",
        "    # below is the code for 1/1+e^(-(bo*x1+ b1*x2 +  b1*x3............ ))\n",
        "    #return  horizontal array\n",
        "    return 1.0/(1 + np.exp(-np.dot(X, beta.T))) \n",
        "  \n",
        "  \n",
        "def log_gradient(beta, X, y): \n",
        "    ''' \n",
        "    logistic gradient function \n",
        "    '''\n",
        "    #first_calc = y_prediction - y_actual  for all samples # make y_actual as\n",
        "    first_calc = logistic_func(beta, X) - y.reshape(X.shape[0], -1) \n",
        "    # now in below step we will find the partial derivative\n",
        "    #final_calc= gradient is (y_prediction - y_actual)*x  for all samples \n",
        "    final_calc = np.dot(first_calc.T, X) \n",
        "    \n",
        "    return final_calc \n",
        "  \n",
        "  \n",
        "def cost_func(beta, X, y): \n",
        "    ''' \n",
        "    cost function, J \n",
        "    '''\n",
        "    #y_prediction=  1/1+e^(-(bo*x1+ b1*x2 +  b1*x3............ )) for all samples\n",
        "    y_prediction= logistic_func(beta, X) \n",
        "    y = np.squeeze(y) \n",
        "    # calculate cross entropy cost function for all samples\n",
        "    cost_function = -(y * np.log(y_prediction)) - ((1 - y) * np.log(1 - y_prediction) ) \n",
        "    # return the sum of  cost function divided by no. of samples\n",
        "    return np.mean(cost_function) \n",
        "  \n",
        "  \n",
        "def train(X, y, beta, lr=.01, converge_change=.001): \n",
        "    ''' \n",
        "    gradient descent function \n",
        "    '''\n",
        "    cost = cost_func(beta, X, y) \n",
        "    change_cost = 1\n",
        "    num_iter = 1\n",
        "      \n",
        "    while(change_cost > converge_change): \n",
        "        old_cost = cost \n",
        "        #beta= beta - learning_rate * partial derivative of cost function w.r.t beta\n",
        "        beta = beta - (lr * log_gradient(beta, X, y)) \n",
        "        # again calculate cost function\n",
        "        cost = cost_func(beta, X, y) \n",
        "        # find difference between old cost and new cost \n",
        "        #if change is greater than .001 then reiterate \n",
        "        change_cost = old_cost - cost \n",
        "        num_iter += 1\n",
        "      \n",
        "    return beta, num_iter  \n",
        "  \n",
        "  \n",
        "def pred_values(beta, X): \n",
        "    ''' \n",
        "    function to predict labels \n",
        "    '''\n",
        "    pred_prob = logistic_func(beta, X) \n",
        "    pred_value = np.where(pred_prob >= .5, 1, 0) \n",
        "    return np.squeeze(pred_value) \n",
        "  \n",
        "  \n",
        "def plot_reg(X, y, beta): \n",
        "    ''' \n",
        "    function to plot decision boundary \n",
        "    '''\n",
        "    # labelled observations \n",
        "    x_0 = X[np.where(y == 0.0)] \n",
        "    x_1 = X[np.where(y == 1.0)] \n",
        "      \n",
        "    # plotting points with diff color for diff label \n",
        "    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') \n",
        "    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') \n",
        "      \n",
        "    # plotting decision boundary \n",
        "    x1 = np.arange(0, 1, 0.1) \n",
        "    x2 = -(beta[0,0] + beta[0,1]*x1)/beta[0,2] \n",
        "    plt.plot(x1, x2, c='k', label='reg line') \n",
        "  \n",
        "    plt.xlabel('x1') \n",
        "    plt.ylabel('x2') \n",
        "    plt.legend() \n",
        "    plt.show() \n",
        "      \n",
        "  \n",
        "      \n",
        "if __name__ == \"__main__\": \n",
        "    # load the dataset \n",
        "    dataset = loadCSV('dataset1.csv') \n",
        "      \n",
        "    # normalizing feature matrix \n",
        "    X = normalize(dataset[:, :-1]) \n",
        "    print(X)\n",
        "      \n",
        "    # stacking columns wth all ones in feature matrix \n",
        "    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X))\n",
        "    print(X)\n",
        "  \n",
        "    # response vector \n",
        "    y = dataset[:, -1] \n",
        "  \n",
        "    # initial beta values \n",
        "    beta = np.matrix(np.zeros(X.shape[1])) \n",
        "  \n",
        "    # beta values after running gradient descent \n",
        "    beta, num_iter =train(X, y, beta) \n",
        "  \n",
        "    # estimated beta values and number of iterations \n",
        "    print(\"Estimated regression coefficients:\", beta) \n",
        "    print(\"No. of iterations:\", num_iter) \n",
        "  \n",
        "    # predicted labels \n",
        "    y_pred = pred_values(beta, X) \n",
        "      \n",
        "    # number of correctly predicted labels \n",
        "    print(\"Correctly predicted labels:\", np.sum(y == y_pred)) \n",
        "      \n",
        "    # plotting regression line \n",
        "    plot_reg(X, y, beta) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}